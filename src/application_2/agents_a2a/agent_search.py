# agent_search.py â€” OpenAI-only: genera un resumen "tipo bÃºsqueda" y SIEMPRE devuelve {"internet_text": "..."}
from __future__ import annotations
import asyncio, json, traceback, os, logging
from typing import Dict, Any

from python_a2a import (
    A2AServer, Message, TextContent, MessageRole,
    run_server, AgentCard, AgentSkill
)
from langgraph.checkpoint.memory import InMemorySaver
from langchain_openai import ChatOpenAI

# --------- logging bÃ¡sico ----------
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
log = logging.getLogger("AgentSearch")

# --------- helper sync/async ----------
def run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(coro)
    else:
        import threading
        box = {}
        def _runner():
            nl = asyncio.new_event_loop()
            asyncio.set_event_loop(nl)
            try:
                box["res"] = nl.run_until_complete(coro)
            finally:
                nl.close()
        t = threading.Thread(target=_runner, daemon=True); t.start(); t.join()
        return box.get("res")

# --------- clase del agente ----------
class SearchA2A(A2AServer):
    def __init__(self, host: str = "127.0.0.1", port: int = 8001):
        skill = AgentSkill(
            id="search",
            name="search",
            description="Genera un breve resumen tipo 'resultado de bÃºsqueda' con OpenAI"
        )
        card = AgentCard(
            name="Agent Search",
            description="Agente de bÃºsqueda (simulada) vÃ­a OpenAI; salida normalizada",
            url=f"http://{host}:{port}",    # IMPORTANTE: sin '/a2a' aquÃ­
            version="1.0.0",
            skills=[skill],
            authentication=None
        )
        super().__init__(agent_card=card)
        self._memory = InMemorySaver()

        # Usa el modelo de env si estÃ¡ definido; por defecto gpt-4o-mini
        model_id = "gpt-4o-mini"
        if not os.getenv("OPENAI_API_KEY"):
            log.warning("OPENAI_API_KEY no estÃ¡ definida en este proceso. El agente podrÃ­a fallar.")
        self._llm = ChatOpenAI(model=model_id, temperature=0.1)

    # --- util: aceptar texto plano o {"query": "..."} ---
    @staticmethod
    def _pick_query(text: str) -> str:
        try:
            obj = json.loads(text or "")
            if isinstance(obj, dict) and obj.get("query"):
                return str(obj["query"])
        except Exception:
            pass
        return text or ""

    async def _handle_async(self, message: Message) -> Message:
        try:
            raw_in = message.content.text or ""
            query = self._pick_query(raw_in).strip()
            log.info(f"[search] query='{query}' len={len(query)}")

            if not query:
                out = {"internet_text": "[search_error] la consulta llegÃ³ vacÃ­a al agente search."}
                return Message(
                    content=TextContent(text=json.dumps(out, ensure_ascii=False)),
                    role=MessageRole.AGENT,
                    parent_message_id=message.message_id,
                    conversation_id=message.conversation_id
                )

            # Prompt: estilo "resultado de bÃºsqueda" (sin inventar links)
            system = (
                "Eres un asistente que redacta un resumen estilo 'resultado de bÃºsqueda'. "
                "SÃ© conciso (3â€“6 lÃ­neas), neutral y Ãºtil. No inventes enlaces ni datos dudosos. "
                "Si la informaciÃ³n puede variar, aclÃ¡ralo brevemente."
            )
            user = (
                "Tema/Pregunta del usuario:\n"
                f"{query}\n\n"
                "Escribe un breve resumen informativo y prÃ¡ctico (3â€“6 lÃ­neas)."
            )

            # Llamada al modelo (con timeout defensivo)
            try:
                resp = await asyncio.wait_for(
                    self._llm.ainvoke([{"role": "system", "content": system},
                                       {"role": "user", "content": user}]),
                    timeout=120
                )
                text = (resp.content or "").strip()
                if not text:
                    text = "[search_error] el modelo devolviÃ³ contenido vacÃ­o."
            except asyncio.TimeoutError:
                text = "[search_error] timeout consultando al modelo OpenAI."
            except Exception as e:
                log.exception("Fallo OpenAI en Search")
                text = f"[search_error] fallo OpenAI: {e}"

            payload = json.dumps({"internet_text": text}, ensure_ascii=False)
            print(f"############## payload ##############\n{payload}")
            return Message(
                content=TextContent(text=payload),
                role=MessageRole.AGENT,
                parent_message_id=message.message_id,
                conversation_id=message.conversation_id
            )

        except Exception as e:
            err = {"error": str(e), "trace": traceback.format_exc()}
            return Message(
                content=TextContent(text=json.dumps(err, ensure_ascii=False)),
                role=MessageRole.AGENT,
                parent_message_id=message.message_id,
                conversation_id=message.conversation_id
            )

    def handle_message(self, message: Message) -> Message:
        return run_async(self._handle_async(message))

# --------- main ----------
if __name__ == "__main__":
    host, port = "127.0.0.1", 8001
    print(f"ðŸš€ Agent A2A Search (OpenAI) escuchando en http://{host}:{port}")
    run_server(SearchA2A(host=host, port=port), host=host, port=port)
